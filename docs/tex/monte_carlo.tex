\documentclass{article}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{pgffor}
\usepackage{amssymb}
\usepackage{bm}
\usepackage[a4paper]{geometry}

\def\set#1{%
    \ensuremath{%
        \ifx!#1!\emptyset\else
            \{%
                \foreach[count=\i] \x in {#1}{%
                    \ifnum\i>1,\,\fi%
                    \x%
                }%
                \,
            \}
        \fi%
    }%
}

\allowdisplaybreaks
\renewcommand\qedsymbol{QED}
\newtheorem{theorem}{Theorem}
\newtheorem{axiom}{Axiom}
\newtheorem{lemma}[theorem]{Lemma}
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}

\begin{document}
  \begin{center}
    \section*{Notes on Monte Carlo}
  \end{center}
  
  The following are definitions and derivations that 
  are explicitly written down with an extent of rigor
  that I personally find useful as it regards the implementation.

  \section{Monte Carlo Estimator}

  We intend to probabilistically approximate the integral $\int_{D^\ast}f$ for some 
  integrable function $f\,:\,D\to R$, where $D^\ast \subseteq D$.
  The Monte Carlo estimator will suffice:
  given $n$ iid samples $\bm{X}_i \in D^\ast$ s.t. $\bm{X}_i \sim p \implies 
  p(D \setminus D^\ast) = 0$ 
  (with the usual restriction that $\int_{D} p = 1$), we define our estimator 
  \begin{align*}
    &\bm{M}_n = \frac{1}{n}\sum_{i=1}^{n} \frac{f(\bm{X}_i)}{p(\bm{X}_i)}\\
    \implies &\begin{cases}
      \E[\bm{M}_n] &= \int_{D}p(\bm{x})\cdot\frac{1}{n}\sum_{i=1}^{n} \frac{f(\bm{x})}{p(\bm{x})}\,d\bm{x}\\
                   &= \frac{1}{n}\sum_{i=1}^{n}
                      \int_{D^\ast}\frac{f(\bm{x})}{p(\bm{x})} p(\bm{x})\,d\bm{x}\\
                   &= \int_{D^\ast}f = \mu\\
      \Var[\bm{M}_n] &= \Var[\frac{1}{n}\sum_{i=1}^{n} \frac{f(\bm{X}_i)}{p(\bm{X}_i)}]\\
                     &= \frac{1}{n^2}\sum_{i=1}^{n}\Var[f(\bm{X}_i) / p(\bm{X}_i)]\\
                     &= \frac{1}{n}\Var[f(\bm{X}_i) / p(\bm{X}_i)]
    \end{cases}
  \end{align*}
  Thus $\lim_{n \to \infty} \Var[\bm{M}_n] = 0$. 
  The definition of variance suggests that 
  increasing the number of samples reduces 
  squared error: $\Var[\bm{M}_n] = \E[(\bm{M}_n - \mu)^2]$, 
  which in turn suggests that the estimator converges to the desired integral 
  (which could perhaps be rationalized as a consequence of the law of large numbers).

  \section{Improving Estimator Efficiency}
  \subsection{Importance Sampling}
    Suppose we pick $p$ s.t. $p = kf$, where $f$ is the estimated function from before.
    Then $\int_{D}p = 1 \implies k = 1 / \int_{D^\ast}f$,
    in which case the estimator term $\frac{f(\bm{X}_i)}{p(\bm{X}_i)} = \int_{D^\ast} f = \mu$ already.
    Then $\Var[\bm{M}_n] = 0$ immediately.
    While this ideal $p$ defeats the purpose of the Monte Carlo estimator,
    it intuitively follows that picking $p$ that roughly 
    conforms to the ``shape'' of $f$ will decrease estimator variance.
    In practice, this means making $p$ large when the contribution from $f$ is 
    large and vice-versa for when the contribution from $f$ is relatively small.
  \subsection{Multiple Importance Sampling (MIS)}
    It may be desireable to utilize multiple densities $p_i$ 
    when estimating the rendering equation. 
    Veach et al (1997) offers the multi-sample Monte Carlo estimator:
    \begin{align*}
      \bm{M}_n^\ast = \sum_{i=1}^{n} \frac{1}{n_i} 
                      \sum_{j=1}^{n} w_i(\bm{X}_{i,\,j})\frac{f(\bm{X}_{i,\,j})}{p_i(\bm{X}_{i,\,j})}
    \end{align*}
    given a set of densities $\set{p_1,\dots,p_n}$ and $n_i$ samples 
    drawn for each $p_i$ and $\bm{X}_{i,\,j} \sim p_i$.

    We expect that the bias, $\beta(\bm{M}) = \E[\bm{M}] - \int_{D^\ast}f$ is still zero 
    so long as we impose the conditions that 
    \textbf{(W1)} $\sum_{i=1}^{n}w_i(\bm{x}) = 1$ when $f(\bm{x})\neq0$ and 
    \textbf{(W2)} $w_i(\bm{x}) = 0$ when $p_i(\bm{x}) = 0$:
    \begin{proof}[The multi-sample estimator is unbiased: $\beta(\bm{M}_n^\ast) = 0$]
      Each random sample $\bm{X}_{i,\,j}$ is not necessarily 
      identically-distributed but they are nevertheless independent, 
      so we can manipulate the expectation accordingly,
      assuming each $n_i \geq 1$:
      \begin{align*}
        \E[\bm{M}_n^\ast] &= 
        \int_{D}\sum_{i=1}^{n} \frac{1}{n_i} 
        \sum_{j=1}^{n} w_i(\bm{x})\frac{f(\bm{x})}{p_i(\bm{x})}\cdot p_i(\bm{x})\,d\bm{x}\\
        &= \int_{D^\ast}\sum_{i=1}^{n} w_i(\bm{x})f(\bm{x})\,d\bm{x}\\
        &= \int_{D^\ast}f, \text{ by \textbf{(W1)}}
      \end{align*}
    \end{proof}
    Veach et al offers the power heuristic as a ``good'' 
    weighting function: $w_i(\bm{x}) = \frac{[n_ip_i(\bm{x})]^\gamma}{\sum_{k}[n_kp_k(\bm{x})]^\gamma}$,
    where $\gamma = 1$ produces the simpler balance heuristic ($\gamma = 2$ is 
    often sufficient). And it is clear that 
    the power heuristic meets both weight function criteria.

  \subsection{Russian Roulette}
    Russian roulette offers a way to terminate paths while 
    maintaining an unbiased estimate. 
    After picking an arbitrary termination probability 
    $q\in [0, 1]$ (usually increasing as the integrand becomes smaller), we 
    define a new discrete estimator 
    $\bm{R} \in \set{\frac{1}{1-q}\bm{M}_n^\ast, \vec{0}}$
    s.t. $P(\bm{R} = \frac{1}{1-q}\bm{M}_n^\ast) = 1-q$ and 
    $P(\bm{R} = \vec{0}) = q$.
    Then 
    \begin{align*}
      \E[\bm{R}] &= (1-q)\cdot \frac{1}{1-q}\E[\bm{M}_n^\ast] + q \cdot \vec{0}
      = \E[\bm{M}_n^\ast]
    \end{align*}

  \section{Light Transport}
  \subsection{Rendering Equation}
    Radiance is flux per unit projected area per unit solid angle 
    (watts/(steradian$\cdot$m$^2$)),
    which is what we seek to measure.
    The rendering equation describes 
    outgoing radiance from a point $\bm{x}$ 
    in a direction $\bm{\Theta}$: 
    \begin{align*}
      L(\bm{x}\to \bm{\Theta}) = L_e(\bm{x}\to\bm{\Theta}) + 
      \int_{\Omega_{\bm{x}}}f_r(\bm{x},\bm{\Psi}\to\bm{\Theta})
      L(\bm{x}\gets \bm{\Psi})|\bm{N_x}\cdot \bm{\Psi}|\,d\bm{\omega_\bm{\Psi}}
    \end{align*}
    Given incoming direction(s) $\bm{\Psi}$, BRDF $f_r$, 
    incoming radiance $L$, emitted radiance $L_e$, 
    and surface normal $\bm{N_x}$.
    Alternatively, the 
    area formulation of the rendering equation 
    states that 
    \begin{align*}
      L(\bm{x}\to \bm{\Theta}) = L_e(\bm{x}\to\bm{\Theta}) + 
      \int_{A}f_r(\bm{x},\bm{\Psi}\to\bm{\Theta})L(\bm{y}\to -\bm{\Psi})
      V(\bm{x},\bm{y})\frac{|\bm{N_x}\cdot\bm{\Psi}||\bm{N_y}\cdot -\bm{\Psi}|}{r_{\bm{xy}}^2}\,dA_{\bm{y}}
    \end{align*}
    because incoming radiance is 
    equivalent to ougoing radiance from 
    every other point $\bm{y}$ in the scene,
    with the visibility term $V$ to account for 
    obstructions.
    The area formulation enables importance sampling of 
    light sources in a scene,
    so it is useful to use it for ``direct'' illumination 
    and the preceding hemispherical formulation for ``indirect'' 
    illumination:
    \begin{align*}
      L_r(\bm{x}\to\bm{\Theta}) = 
      &\int_{A}f_r(\bm{x}, \bm{\Theta}\to\bm{\Psi})L_e(y\to-\bm{\Psi})
      V(\bm{x},\bm{y})G(\bm{x},\bm{y})\,dA_{\bm{y}}
      +\\
      &\int_{\omega_{\bm{x}}}f_r(\bm{x}, \bm{\Theta}\to\bm{\Psi})
      L_i(x\gets \bm{\Psi})|\bm{N_x}\cdot\bm{\Psi}|\,d\omega_{\bm{\Psi}}
    \end{align*}
    where $L_i$ is reflected radiance from the incoming direction $\bm{\Psi}$.
    The light transport problem is suitable for Monte Carlo integration.
\end{document}